{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from keybert import KeyBERT\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_url = \"https://www.cc.gatech.edu/\"\n",
    "\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "60\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "63\n",
      "64\n",
      "65\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "69\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "75\n",
      "76\n",
      "76\n",
      "76\n",
      "76\n",
      "76\n",
      "76\n",
      "76\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "78\n",
      "78\n",
      "79\n",
      "80\n",
      "80\n",
      "81\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "85\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "90\n",
      "90\n",
      "91\n",
      "91\n",
      "92\n",
      "93\n",
      "93\n",
      "93\n",
      "93\n",
      "94\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "visited = set()\n",
    "q = [starting_url]\n",
    "storage = collections.defaultdict(list)\n",
    "\n",
    "while len(q) > 0 and len(visited) < 1000:\n",
    "    url = q.pop(0)\n",
    "    if url not in visited:\n",
    "        try: \n",
    "            x = requests.get(url, timeout=5)\n",
    "\n",
    "            if x.status_code == 200:\n",
    "                visited.add(url)\n",
    "                print(len(visited))\n",
    "                soup = BeautifulSoup(x.text, 'html.parser')\n",
    "                relative_links = soup.find_all('a')\n",
    "                text = soup.get_text(' ', strip=True)[:5000]\n",
    "                keywords = model.extract_keywords(text)\n",
    "                for kw in keywords:\n",
    "                    storage[kw].append(url)\n",
    "\n",
    "                for link in relative_links:\n",
    "                    next_url = link.get(\"href\")\n",
    "                    if next_url and \"cc.gatech.edu\" in next_url and next_url not in visited:\n",
    "                        q.append(next_url)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(len(visited))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
